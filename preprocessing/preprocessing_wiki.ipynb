{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z35M7FVPeUfK"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install flair\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# load tagger\n",
        "tagger = SequenceTagger.load(\"flair/ner-english-large\")"
      ],
      "metadata": {
        "id": "zhRiJwXGecbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anonymizer\n",
        "from anonymizer import entity\n",
        "from anonymizer.core import initialize\n",
        "from anonymizer.cache import NECache\n",
        "import anonymizer.entity.person as person\n",
        "from anonymizer.entity.org import org, org_wiki\n",
        "from anonymizer.entity.gpe import gpe, gpe_wiki\n",
        "from functools import partial\n",
        "initialize()"
      ],
      "metadata": {
        "id": "AMKGZrKwedL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_entities_flair_wiki(text):\n",
        "    # make example sentence\n",
        "    sentence = Sentence(text)\n",
        "    # predict NER tags\n",
        "    tagger.predict(sentence)\n",
        "    # iterate over entities and print\n",
        "    replacements = []\n",
        "    replacement_map = {}\n",
        "    if not sentence.get_spans('ner'):\n",
        "        return text\n",
        "\n",
        "    for entity in sentence.get_spans('ner'):\n",
        "        if entity.text in replacement_map:\n",
        "            replacements.append((entity.start_position, entity.end_position, replacement_map[entity.text], entity.text))\n",
        "            continue\n",
        "        if entity.get_label().value == \"ORG\":\n",
        "            repl = org.handle(entity.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                repl = org.handle(entity.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                continue\n",
        "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
        "            replacement_map[entity.text] = \" \".join(repl)\n",
        "        elif entity.get_label().value == \"PER\":\n",
        "            repl = person.handle(entity.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                repl = person.handle(entity.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                continue\n",
        "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
        "            replacement_map[entity.text] = \" \".join(repl)\n",
        "        elif entity.get_label().value == \"LOC\":\n",
        "            repl = gpe.handle([entity.text], {})\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                repl = gpe.handle([entity.text], {})\n",
        "            if not repl or \" \".join(repl) == entity.text:\n",
        "                continue\n",
        "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
        "            replacement_map[entity.text] = \" \".join(repl)\n",
        "\n",
        "    if replacements:\n",
        "        res = []\n",
        "        i = 0\n",
        "        s = text\n",
        "        for (start, end, txt, orig) in replacements:\n",
        "            assert orig != txt\n",
        "            res.append(s[i:start] + txt)\n",
        "            i = end\n",
        "        res.append(s[end:])\n",
        "        return ''.join(res)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NU2d4uuPel5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_data = load_dataset(\"imdb\")\n",
        "train_data = cls_data['train']\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "x1U9pJ3terdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"imdb_train_flair_wiki.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"text\",\"label\"])\n",
        "    for p in tqdm(train_data):\n",
        "        src = replace_entities_flair_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
        "        writer.writerow((src, p['label']))"
      ],
      "metadata": {
        "id": "5xaQ4wMne9KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_data = load_dataset(\"cnn_dailymail\")\n",
        "train_data = cls_data['train']\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "HCJOegd3fkJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cnn_dm_train_flair_wiki.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"article\",\"highlights\"])\n",
        "    for p in tqdm(train_data):\n",
        "        src = replace_entities_flair_wiki(p['article'])\n",
        "        trg = replace_entities_flair_wiki(p['highlights'])\n",
        "        writer.writerow((src, trg))"
      ],
      "metadata": {
        "id": "uRmRR7Lkflr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a22b859"
      },
      "source": [
        "## Spacy wiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0ba6faff"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d826bb0e"
      },
      "outputs": [],
      "source": [
        "def replace_entities_spacy_wiki(text):\n",
        "    parsed = nlp(text)\n",
        "    # iterate over entities and print\n",
        "    replacements = []\n",
        "    replacement_map = {}\n",
        "    if all([w.ent_type == 0 for w in parsed]):\n",
        "        return text\n",
        "\n",
        "    for word in parsed:\n",
        "        if word.text in replacement_map:\n",
        "            replacement_map[\"-\"] = \"-\"\n",
        "            replacements.append((word.idx, word.idx + len(word.text), replacement_map[word.text], word.text))\n",
        "            continue\n",
        "        if word.ent_type_ == \"ORG\":\n",
        "            repl = org.handle(word.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                repl = org.handle(word.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                continue\n",
        "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
        "            replacement_map[word.text] = \" \".join(repl)\n",
        "        elif word.ent_type_ == \"PERSON\":\n",
        "            repl = person.handle(word.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                repl = person.handle(word.text.split(\" \"), NECache())\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                continue\n",
        "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
        "            replacement_map[word.text] = \" \".join(repl)\n",
        "        elif word.ent_type_ == \"GPE\":\n",
        "            repl = gpe.handle([word.text], {})\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                repl = gpe.handle([word.text], {})\n",
        "            if not repl or \" \".join(repl) == word.text:\n",
        "                continue\n",
        "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
        "            replacement_map[word.text] = \" \".join(repl)\n",
        "            replacement_map[\"-\"] = \"-\"\n",
        "\n",
        "    if replacements:\n",
        "        res = []\n",
        "        i = 0\n",
        "        for (start, end, txt, orig) in replacements:\n",
        "            assert orig != txt\n",
        "            res.append(text[i:start] + txt)\n",
        "#             print(\"\\\"\" + text[i:start] + \"\\\"\", \"\\\"\" + orig + \"\\\"\", \"\\\"\" + txt + \"\\\"\")\n",
        "            i = end\n",
        "        res.append(text[end:])\n",
        "        return ''.join(res)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_data = load_dataset(\"imdb\")\n",
        "train_data = cls_data['train']\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "GoSMPtJMf8H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2e0f816f"
      },
      "outputs": [],
      "source": [
        "with open(\"imdb_train_spacy_wiki.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"text\",\"label\"])\n",
        "    for p in tqdm(train_data):\n",
        "        src = replace_entities_spacy_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
        "        writer.writerow((src, p['label']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"imdb_train_spacy_wiki.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"text\",\"label\"])\n",
        "    for p in tqdm(train_data):\n",
        "        src = replace_entities_spacy_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
        "        writer.writerow((src, p['label']))"
      ],
      "metadata": {
        "id": "4Tb8PVUBf5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_data = load_dataset(\"cnn_dailymail\")\n",
        "train_data = cls_data['train']\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "hhWb5YNjf5Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cnn_dm_train_spacy_wiki.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"article\",\"highlights\"])\n",
        "    for p in tqdm(train_data):\n",
        "        src = replace_entities_spacy_wiki(p['article'])\n",
        "        trg = replace_entities_spacy_wiki(p['highlights'])\n",
        "        writer.writerow((src, trg))"
      ],
      "metadata": {
        "id": "bktO118kf5Cp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}